{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ProjectB Colab Notebook\n",
        "This notebook combines code from `projectB.py`, `utils.py`, and `variational_network.py`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport matplotlib.pyplot as plt\nimport scipy.io"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Utility Functions"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import numpy as np\nimport torch\nimport torch.nn.functional as F\n\ndef k2i(img, dims=(0, )):\n    dim_img = img.shape\n    if dims is None:\n        factor = np.prod(dim_img)\n        return np.sqrt(factor) * np.fft.fftshift(np.fft.ifftn(np.fft.ifftshift(img)))\n    else:\n        for dim in dims:\n            img = np.sqrt(dim_img[dim]) * np.fft.fftshift(\n                np.fft.ifft(np.fft.ifftshift(img, axes=dim), axis=dim), axes=dim\n            )\n        return img\n\ndef i2k(img, dims=(0, )):\n    dim_img = img.shape\n    if dims is None:\n        factor = np.prod(dim_img)\n        return (1/np.sqrt(factor)) * np.fft.fftshift(np.fft.fftn(np.fft.ifftshift(img)))\n    else:\n        for dim in dims:\n            img = (1/np.sqrt(dim_img[dim])) * np.fft.fftshift(\n                np.fft.fft(np.fft.ifftshift(img, axes=dim), axis=dim), axes=dim\n            )\n        return img\n\ndef vtv_loss(x):\n    \"\"\"\n    Computes the Vectorial Total Variation (VTV) loss as described in Eq. (19) of the script.\n\n    Args:\n        x (torch.Tensor): A dynamic image sequence of shape (T, H, W),\n                          where T is the number of frames, H is the height, and W is the width.\n\n    Returns:\n        torch.Tensor: The VTV loss value (scalar).\n    \"\"\"\n    T, H, W = x.shape\n\n    # Compute spatial gradients for each frame (along spatial axes)\n    grad_x = x[:, :, 1:] - x[:, :, :-1]       # horizontal gradient (along width)\n    grad_y = x[:, 1:, :] - x[:, :-1, :]       # vertical gradient (along height)\n\n    # Pad to (T, H, W) for consistent size\n    grad_x = F.pad(grad_x, (0, 1), mode='replicate')   # pad last column (W dimension)\n    grad_y = F.pad(grad_y, (0, 0, 0, 1), mode='replicate')  # pad last row (H dimension)\n\n    # Compute squared gradient magnitude across time (sum over T)\n    squared_gradients = grad_x**2 + grad_y**2\n    vtv_map = squared_gradients.sum(dim=0)  # sum over T, shape: (H, W)\n\n    # Take sqrt and sum over spatial dimensions\n    vtv = torch.sqrt(vtv_map + 1e-8).sum()\n\n    return vtv\n\ndef generate_undersampling_mask(shape, acceleration, center_fraction=0.1, sigma=10):\n    \"\"\"\n    Generates a 3D undersampling mask with incoherent sampling across time and a Laplace-shaped density.\n\n    Args:\n        shape: (H, W, T) \u2014 dimensions of k-space data\n        acceleration: desired acceleration factor (e.g., 4 or 6)\n        center_fraction: fraction of central k-space to fully sample\n        sigma: width parameter for Laplace-shaped density (controls decay)\n\n    Returns:\n        mask: numpy array of shape (H, W, T) with 0 (unsampled) or 1 (sampled)\n    \"\"\"\n    H, W, T = shape\n    mask = np.zeros((H, W, T), dtype=np.float32)\n\n    # Sample the center of k-space\n    center_size_h = int(H * center_fraction)\n    center_size_w = int(W * center_fraction)\n    ch_start, ch_end = H//2 - center_size_h//2, H//2 + center_size_h//2\n    cw_start, cw_end = W//2 - center_size_w//2, W//2 + center_size_w//2\n\n    mask[ch_start:ch_end, cw_start:cw_end, :] = 1  # fully sample center for all time frames\n\n    # Create Laplace-shaped density for outer k-space sampling\n    ky = np.arange(H) - H//2\n    kx = np.arange(W) - W//2\n    ky_grid, kx_grid = np.meshgrid(ky, kx, indexing='ij')\n    distance = np.sqrt(ky_grid**2 + kx_grid**2)\n\n    laplace_density = np.exp(-distance / sigma)\n    laplace_density[ch_start:ch_end, cw_start:cw_end] = 0  # exclude fully sampled center\n\n    # Normalize to sum to 1\n    laplace_density /= laplace_density.sum()\n\n    num_samples = int(H * W / acceleration) - (center_size_h * center_size_w)\n\n    for t in range(T):\n        # Flatten the density and sample points according to the probability\n        flat_density = laplace_density.flatten()\n        sampled_indices = np.random.choice(H * W, num_samples, replace=False, p=flat_density)\n        sampled_coords = np.unravel_index(sampled_indices, (H, W))\n        mask[sampled_coords[0], sampled_coords[1], t] = 1\n\n    return mask\n\ndef compute_noisy_undersampled_measurements(img, mask, sigma=0.01):\n    \"\"\"\n    Computes the noisy undersampled k-space measurements s = M (F x + N(0, sigma I)).\n\n    Args:\n        img (ndarray): Input image in spatial domain, shape (H, W, T)\n        mask (ndarray): Binary undersampling mask, shape (H, W, T)\n        sigma (float): Standard deviation of Gaussian noise\n\n    Returns:\n        s (ndarray): Noisy undersampled k-space data, shape (H, W, T)\n    \"\"\"\n    # Compute Fourier transform of the image\n    kspace_full = i2k(img, dims=(0, 1))  # apply FT along spatial dimensions only (H, W)\n\n    # Add Gaussian noise (same shape as k-space)\n    noise = np.random.normal(0, sigma, kspace_full.shape) + 1j * np.random.normal(0, sigma, kspace_full.shape)\n    kspace_noisy = kspace_full + noise\n\n    # Apply sampling mask (element-wise multiplication)\n    s = mask * kspace_noisy\n\n    return s\n\n\ndef complex_from_tensor(t):\n    \"\"\"Convert a real-valued tensor to a complex-valued tensor.\n\n    The input is expected to have the real/imaginary components stacked in\n    the first dimension, i.e. ``(2, T, H, W)`` or ``(B, 2, T, H, W)`` where ``B``\n    denotes an optional batch dimension.  The returned tensor has the complex\n    dimension removed and the time dimension moved to the end resulting in\n    shapes ``(H, W, T)`` or ``(B, H, W, T)`` respectively.\n    \"\"\"\n\n    if t.ndim == 4:\n        # (2, T, H, W) -> (H, W, T)\n        return torch.view_as_complex(t.permute(2, 3, 1, 0).contiguous())\n    elif t.ndim == 5:\n        # (B, 2, T, H, W) -> (B, H, W, T)\n        return torch.view_as_complex(t.permute(0, 3, 4, 2, 1).contiguous())\n    else:\n        raise ValueError(\n            \"Tensor must have shape (2, T, H, W) or (B, 2, T, H, W)\")\n\n\ndef tensor_from_complex(c):\n    \"\"\"Convert complex tensor to a real-valued representation.\n\n    Accepts a tensor of shape ``(H, W, T)`` or ``(B, H, W, T)`` and returns a\n    real tensor with the complex dimension stacked in the first position,\n    resulting in shapes ``(2, T, H, W)`` or ``(B, 2, T, H, W)`` respectively.\n    \"\"\"\n\n    real = torch.view_as_real(c)\n    if c.ndim == 3:\n        # (H, W, T, 2) -> (2, T, H, W)\n        return real.permute(3, 2, 0, 1)\n    elif c.ndim == 4:\n        # (B, H, W, T, 2) -> (B, 2, T, H, W)\n        return real.permute(0, 4, 3, 1, 2)\n    else:\n        raise ValueError(\n            \"Complex tensor must have shape (H, W, T) or (B, H, W, T)\")\n\n\ndef i2k_torch(x):\n    \"\"\"Fourier transform from image to k-space.\n\n    The function operates on tensors with shape ``(..., T, H, W)`` where the\n    last three dimensions correspond to time, height and width.  The Fourier\n    transform is applied over the spatial dimensions resulting in an output of\n    shape ``(..., H, W, T)``.\n    \"\"\"\n\n    # Move the time dimension to the end so that H and W are the last two dims\n    x_c = torch.movedim(x, -3, -1)\n    k = torch.fft.fftn(\n        torch.fft.ifftshift(x_c, dim=(-3, -2)),\n        dim=(-3, -2),\n        norm=\"ortho\",\n    )\n    k = torch.fft.fftshift(k, dim=(-3, -2))\n    return k\n\n\ndef k2i_torch(k):\n    \"\"\"Inverse Fourier transform from k-space to image.\n\n    Accepts tensors with shape ``(..., H, W, T)`` and returns real-valued image\n    tensors with shape ``(..., T, H, W)``.  The inverse transform is performed\n    over the spatial dimensions.\n    \"\"\"\n\n    img = torch.fft.ifftn(\n        torch.fft.ifftshift(k, dim=(-3, -2)),\n        dim=(-3, -2),\n        norm=\"ortho\",\n    )\n    img = torch.fft.fftshift(img, dim=(-3, -2))\n    img = torch.movedim(img, -1, -3).real\n    return img\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Variational Network"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass VariationalNetwork(nn.Module):\n    def __init__(self, n_layers=10, n_filters=1, filter_size=3):\n        super(VariationalNetwork, self).__init__()\n        self.n_layers = n_layers\n\n        # Learnable step sizes and regularization weights\n        self.alpha = nn.ParameterList([nn.Parameter(torch.tensor(0.1)) for _ in range(n_layers)])\n        self.mu = nn.ParameterList([nn.Parameter(torch.tensor(0.9)) for _ in range(n_layers)])\n        \n        self.filters = nn.Parameter(torch.randn(n_layers, n_filters, filter_size, filter_size) * 0.01)\n        \n        # activation function (lookup table)\n        self.activation_grid = torch.linspace(0, 1.5, steps=100)  # shape: (G,)\n        self.activation_values = nn.Parameter(torch.rand(n_layers, n_filters, 100))  # shape: (K, F, G)\n\n    def apply_activation(self, x, k, i):\n        \"\"\"\n        Interpolate learnable activation function \u03c6^{k,i} over fixed grid\n        Args:\n            x: input tensor (H, W)\n            k: layer index\n            i: filter index\n        Returns:\n            activated: same shape as x\n        \"\"\"\n        grid = self.activation_grid.to(x.device)  # shape (G,)\n        values = self.activation_values[k, i, :].to(x.device)  # shape (G,)\n\n        # Normalize z to grid range [0, 1.5]\n        z_clamped = x.clamp(min=0, max=1.5)\n\n        idx = torch.bucketize(z_clamped.reshape(-1), grid)\n        idx = torch.clamp(idx, 1, grid.numel() - 1)\n        x0 = grid[idx - 1]\n        x1 = grid[idx]\n        y0 = values[idx - 1]\n        y1 = values[idx]\n        slope = (y1 - y0) / (x1 - x0)\n        activated = y0 + slope * (z_clamped.reshape(-1) - x0)\n        return activated.reshape(z_clamped.shape)\n    \n        \n    def reg_vtv(self, x, k):\n        \"\"\"Compute the Vectorial Total Variation regularizer.\n\n        The method supports inputs of shape ``(T, H, W)`` or ``(B, T, H, W)``.\n        In the batched case the computation is performed independently for each\n        element in the batch.\n        \"\"\"\n\n        batched = x.dim() == 4\n        if batched:\n            B, T, H, W = x.shape\n            reg_term = torch.zeros_like(x)\n            x_reshaped = x.reshape(B * T, 1, H, W)\n        else:\n            T, H, W = x.shape\n            reg_term = torch.zeros_like(x)\n            x_reshaped = x.unsqueeze(1)  # (T,1,H,W)\n\n        for i in range(self.filters.shape[1]):\n            # Get the (k,i)-th filter\n            filt = self.filters[k, i, :, :].unsqueeze(0).unsqueeze(0)  # (1,1,f,f)\n\n            # Convolve each frame (batch-wise convolution)\n            filtered = F.conv2d(\n                x_reshaped,\n                filt,\n                padding=\"same\",\n                groups=1,\n            )\n            if batched:\n                filtered_frames = filtered.view(B, T, H, W)\n            else:\n                filtered_frames = filtered.squeeze(1)  # (T,H,W)\n\n            # Compute joint gradient magnitude across time\n            if batched:\n                magnitude = (filtered_frames ** 2).sum(dim=1) / T  # (B,H,W)\n            else:\n                magnitude = (filtered_frames ** 2).sum(dim=0) / T  # (H,W)\n            magnitude = torch.sqrt(magnitude + 1e-8)\n\n            # Apply learnable activation function\n            phi = self.apply_activation(magnitude, k, i)  # (H, W)\n\n            # Transpose operation: convolution with flipped filter\n            flipped_filt = torch.flip(filt, dims=[2, 3])\n\n            for t in range(T):\n                if batched:\n                    tmp = filtered_frames[:, t] * phi  # (B,H,W)\n                    tmp_conv = F.conv2d(\n                        tmp.unsqueeze(1),\n                        flipped_filt,\n                        padding=\"same\",\n                    ).squeeze(1)  # (B,H,W)\n                    reg_term[:, t] += tmp_conv\n                else:\n                    tmp = filtered_frames[t] * phi  # (H,W)\n                    tmp_conv = F.conv2d(\n                        tmp.unsqueeze(0).unsqueeze(0),\n                        flipped_filt,\n                        padding=\"same\",\n                    ).squeeze(0).squeeze(0)\n                    reg_term[t] += tmp_conv\n\n\n        return reg_term\n\n    def reg_tv(self, x):\n        \"\"\"\n        Isotropic total variation regularizer.\n\n        Args:\n            x: input tensor of shape ``(T, H, W)``\n\n        Returns:\n            reg_term: divergence of normalized gradients with the same shape as ``x``\n        \"\"\"\n\n        grad_x = x[:, :, 1:] - x[:, :, :-1]\n        grad_y = x[:, 1:, :] - x[:, :-1, :]\n\n        grad_x = F.pad(grad_x, (0, 1), mode=\"replicate\")\n        grad_y = F.pad(grad_y, (0, 0, 0, 1), mode=\"replicate\")\n\n        magnitude = torch.sqrt(grad_x ** 2 + grad_y ** 2 + 1e-8)\n        grad_x_norm = grad_x / magnitude\n        grad_y_norm = grad_y / magnitude\n\n        div_x = grad_x_norm - F.pad(grad_x_norm[:, :, :-1], (1, 0), mode=\"replicate\")\n        div_y = grad_y_norm - F.pad(grad_y_norm[:, :-1, :], (0, 0, 1, 0), mode=\"replicate\")\n\n        return div_x + div_y\n\n    def reg_tikhonov(self, x):\n        \"\"\"\n        Tikhonov regularizer implemented via a discrete Laplacian.\n\n        Args:\n            x: input tensor of shape ``(T, H, W)``\n\n        Returns:\n            reg_term: Laplacian of ``x`` with the same shape as ``x``\n        \"\"\"\n\n        kernel = torch.tensor([[0., -1., 0.],\n                              [-1., 4., -1.],\n                              [0., -1., 0.]], device=x.device, dtype=x.dtype)\n        kernel = kernel.view(1, 1, 3, 3)\n        reg_term = F.conv2d(x.unsqueeze(1), kernel, padding=1)\n        return reg_term.squeeze(1)\n    \n    \n\n    def compute_g(self, x, s, M, F, FH, k):\n        \"\"\"\n        Compute g^k = \u03b1^k * F^H(M * (F x^k - s)) + Reg^k(x^k)\n        \n        Args:\n            x: current iterate (T, H, W)\n            s: measured data (H, W, T)\n            M: sampling mask (H, W, T)\n            F: forward operator (function)\n            FH: adjoint operator (function)\n            k: current layer index\n            \n        Returns:\n            g: tensor of shape (T, H, W)\n        \"\"\"\n        # 1. Apply forward Fourier transform to x (output: H, W, T)\n        x_kspace = F(x)  # (H, W, T)\n\n        # 2. Compute k-space residual\n        residual = (M * x_kspace) - s  # (H, W, T)\n\n        # 3. Backproject the residual using inverse Fourier transform\n        data_term = FH(residual)  # (T, H, W)\n\n        # 4. Compute regularization term\n        reg_term = self.reg_vtv(x, k)  # (T, H, W)\n\n        # 5. Combine with step size \u03b1^k\n        g = self.alpha[k] * (data_term + reg_term)  # (T, H, W)\n\n        return g\n    \n    def update_momentum(self, m_prev, g, k):\n        \"\"\"\n        Compute m^{k+1} = \u03bc^{k+1} * m^k + g^k\n\n        Args:\n            m_prev: previous momentum (T, H, W)\n            g: current gradient (T, H, W)\n            k: current layer index\n\n        Returns:\n            m_next: updated momentum (T, H, W)\n        \"\"\"\n        return self.mu[k] * m_prev + g\n\n    def update_x(self, x, m_next):\n        \"\"\"\n        Compute x^{k+1} = x^k - m^{k+1}\n\n        Args:\n            x: current estimate (T, H, W)\n            m_next: updated momentum (T, H, W)\n\n        Returns:\n            x_next: updated image estimate (T, H, W)\n        \"\"\"\n        return x - m_next\n\n\n    def forward(self, x0, s, i2k, k2i, mask, return_intermediate=False):\n\n        x = x0.clone()\n        m = torch.zeros_like(x0)  # Initialize momentum\n\n        xs = []\n        for k in range(self.n_layers):\n\n            g = self.compute_g(x, s, mask, i2k, k2i, k)\n            m = self.update_momentum(m, g, k)\n            x = self.update_x(x, m)\n\n            if return_intermediate:\n                xs.append(x.clone())\n\n        if return_intermediate:\n            return x, xs\n        return x\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Dataset, Training, and Utilities"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "from variational_network import *\nfrom utils import *\nimport scipy.io\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport matplotlib.pyplot as plt\nimport os\n\n#\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Hyperparameters\nN_LAYERS   = 8\nN_FILTERS  = 5\nFILTER_SZ  = 3\n\nNOISE_STD          = 0.01\nACCEL_RATE         = 4\nMASK_CENTER_RADIUS = 8\n\nBATCH_SIZE   = 4\nNUM_EPOCHS   = 20\nLR           = 1e-2\nPRINT_EVERY  = 10\nTRAIN_SPLIT  = 0.8\nDS_TAU       = 0.1\nSHOW_VAL_IMAGES = True\n#\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\nclass HeartDataset(Dataset):\n    \"\"\"Dataset generating noisy, undersampled k-space measurements from\n    the 2dt_heart.mat file.\"\"\"\n\n    def __init__(self, mat_path=\"2dt_heart.mat\", noise_std=NOISE_STD,\n                 acceleration=ACCEL_RATE, center_fraction=0.1, sigma=10):\n        super().__init__()\n        data = scipy.io.loadmat(mat_path)\n        self.imgs = data[\"imgs\"].astype(np.float32)  # (H, W, T, N)\n        self.noise_std = noise_std\n        self.acceleration = acceleration\n        self.center_fraction = center_fraction\n        self.sigma = sigma\n\n    def __len__(self):\n        return self.imgs.shape[3]\n\n    def __getitem__(self, idx):\n        img = self.imgs[..., idx]  # (H, W, T)\n        mask = generate_undersampling_mask(\n            img.shape, self.acceleration, self.center_fraction, self.sigma\n        )\n        s = compute_noisy_undersampled_measurements(img, mask,\n                                                    sigma=self.noise_std)\n\n        img_t = torch.from_numpy(img.transpose(2, 0, 1))  # (T, H, W)\n        real = np.real(s).astype(np.float32).transpose(2, 0, 1)\n        imag = np.imag(s).astype(np.float32).transpose(2, 0, 1)\n        s_t = torch.from_numpy(np.stack([real, imag], axis=0))  # (2, T, H, W)\n        mask_t = torch.from_numpy(mask.transpose(2, 0, 1).astype(np.float32))\n        return img_t, s_t, mask_t\n\n\ndef create_dataloaders(batch_size=BATCH_SIZE, train_split=TRAIN_SPLIT):\n    \"\"\"Utility to create train/validation dataloaders.\"\"\"\n    dataset = HeartDataset()\n    n_train = int(len(dataset) * train_split)\n    n_val = len(dataset) - n_train\n    train_ds, val_ds = random_split(dataset, [n_train, n_val])\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n    return train_loader, val_loader\n\n\n\n\ndef train_vn(num_epochs=NUM_EPOCHS, lr=LR, batch_size=BATCH_SIZE):\n    \"\"\"Train a variational network on the heart dataset and plot the loss.\"\"\"\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    train_loader, _ = create_dataloaders(batch_size=batch_size)\n\n    vn = VariationalNetwork(n_layers=N_LAYERS, n_filters=N_FILTERS,\n                            filter_size=FILTER_SZ).to(device)\n    optim = torch.optim.Adam(vn.parameters(), lr=lr)\n\n    losses = []\n    for epoch in range(num_epochs):\n        vn.train()\n        running = 0.0\n        i = 0\n        for gt, s, m in train_loader:\n            i += 1\n            percent = (i / len(train_loader)) * 100\n            print(f\"Epoch {epoch+1}/{num_epochs} - Progress: {percent:.3f}%\", end='\\r')\n            gt = gt.to(device)\n            s_complex = complex_from_tensor(s).to(device)\n            m = m.permute(0, 2, 3, 1).to(device)\n\n            x0 = k2i_torch(s_complex)\n            pred, preds_all = vn(x0, s_complex, i2k_torch, k2i_torch, m,\n                                 return_intermediate=True)\n\n            plot_loss = torch.mean(torch.abs(torch.abs(pred) - torch.abs(gt)))\n\n            K = len(preds_all)\n            ds_loss = 0.0\n            for k, x_k in enumerate(preds_all, start=1):\n                weight = torch.exp(torch.tensor(-DS_TAU * (K - k), device=gt.device, dtype=torch.float32))\n                ds_loss = ds_loss + weight * F.mse_loss(x_k, gt)\n\n            optim.zero_grad()\n            ds_loss.backward()\n            optim.step()\n            running += plot_loss.item()\n\n        epoch_loss = running / len(train_loader)\n        losses.append(epoch_loss)\n        if (epoch + 1) % PRINT_EVERY == 0:\n            print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.6f}\")\n\n    plt.figure()\n    plt.plot(losses)\n    plt.xlabel('Epoch')\n    plt.ylabel('L1 Loss')\n    plt.title('Training Loss')\n    plt.show()\n    return vn, losses\n\n\ndef validate_vn(model, val_loader=None, batch_size=BATCH_SIZE,\n                display_examples=False, num_examples=3):\n    \"\"\"Validate a trained variational network on the held-out set.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        The trained variational network.\n    val_loader : DataLoader, optional\n        Validation dataloader.  If ``None``, a loader is created using\n        ``create_dataloaders`` with ``batch_size``.\n    batch_size : int\n        Batch size to use when constructing the dataloader.\n    display_examples : bool, optional\n        If ``True``, display ``num_examples`` pairs of ground truth and\n        reconstructed images.\n    num_examples : int, optional\n        Number of example pairs to display when ``display_examples`` is\n        ``True``.\n\n    Returns\n    -------\n    float\n        The average L1 validation loss.\n    \"\"\"\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    if val_loader is None:\n        _, val_loader = create_dataloaders(batch_size=batch_size)\n\n    model = model.to(device)\n    model.eval()\n    running = 0.0\n    examples = []\n    with torch.no_grad():\n        for gt, s, m in val_loader:\n            gt = gt.to(device)\n            s_complex = complex_from_tensor(s).to(device)\n            m = m.permute(0, 2, 3, 1).to(device)\n\n            x0 = k2i_torch(s_complex)\n            pred = model(x0, s_complex, i2k_torch, k2i_torch, m)\n\n            loss = torch.mean(torch.abs(torch.abs(pred) - torch.abs(gt)))\n            running += loss.item()\n\n            if display_examples and len(examples) < num_examples:\n                for j in range(gt.shape[0]):\n                    if len(examples) >= num_examples:\n                        break\n                    examples.append((gt[j].cpu(), pred[j].cpu()))\n\n    if display_examples and examples:\n        n = len(examples)\n        fig, axes = plt.subplots(n, 2, figsize=(6, 3 * n))\n        if n == 1:\n            axes = np.array(axes).reshape(1, -1)\n        for i, (gt_ex, pred_ex) in enumerate(examples):\n            gt_img = torch.abs(gt_ex[0]).numpy()\n            pred_img = torch.abs(pred_ex[0]).numpy()\n            axes[i, 0].imshow(gt_img, cmap=\"gray\")\n            axes[i, 0].set_title(\"Ground Truth\")\n            axes[i, 0].axis(\"off\")\n            axes[i, 1].imshow(pred_img, cmap=\"gray\")\n            axes[i, 1].set_title(\"Reconstruction\")\n            axes[i, 1].axis(\"off\")\n        plt.tight_layout()\n        plt.show()\n\n    return running / len(val_loader)\n\n\ndef save_trained_model(model, directory=\"models\", filename=None, **hyperparams):\n    \"\"\"Save a trained model to ``directory/filename``.\n\n    The filename will encode given hyperparameters if ``filename`` is ``None``.\n\n    Args:\n        model (torch.nn.Module): trained variational network\n        directory (str): directory to store the model file\n        filename (str, optional): file name for the saved state dict.  If not\n            provided, a name is constructed from ``hyperparams``.\n        **hyperparams: keyword arguments describing the hyperparameters used\n            during training.\n    \"\"\"\n\n    def _sanitize(val):\n        if isinstance(val, float):\n            s = f\"{val:.0e}\" if val < 1e-3 or val >= 1e3 else f\"{val:g}\"\n            s = s.replace(\"+\", \"\")\n        else:\n            s = str(val)\n        return s.replace(\".\", \"p\")\n\n    os.makedirs(directory, exist_ok=True)\n\n    if filename is None:\n        parts = [f\"{k}{_sanitize(v)}\" for k, v in sorted(hyperparams.items())]\n        filename = \"vn_\" + \"_\".join(parts) + \".pth\"\n\n    path = os.path.join(directory, filename)\n    torch.save(model.state_dict(), path)\n    print(f\"Model saved to {path}\")\n\n\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
